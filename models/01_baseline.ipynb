{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce0a50-d9b5-4112-80ab-5884731058a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read()\n",
    "# X = df.drop(columns = [])\n",
    "# df['randomY'] = np.random.randint(0,1, size=len(df))\n",
    "# random_y = df['randomY']\n",
    "# y = df[]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data/feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "  \n",
    "\n",
    "    (\"Logistic Regression Classification\", LogisticRegression(), {'C': [1, 2]}),\n",
    "    (\"SVC\", SVC(C = 10), {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}),\n",
    "    (\"Decision Tree Regression\", DecisionTreeClassifier(), {'max_depth':[0,2, 5]}),\n",
    "    (\"Random Forest Classification\", RandomForestClassifier(), {'n_estimators': [100,150,200]}),\n",
    "    (\"Random Forest Classification2\", RandomForestClassifier(min_samples_split = 3,  n_estimators=75), {'n_estimators': [100,150,200]}), \n",
    "    (\"Gradient Boosting Classification\", GradientBoostingClassifier(loss='squared_error', n_estimators=440, max_depth=5, learning_rate=0.012, random_state=0, max_features='sqrt'), {'n_estimators': [100,150,200]}),\n",
    "    (\"Gradient Boosting Classification2\", GradientBoostingClassifier(loss='squared_error', n_estimators=100, max_depth=7, learning_rate=0.2, random_state=0, max_features='sqrt'), {'n_estimators': [100,150,200]}),\n",
    "    (\"Gradient Boosting Classification3\", GradientBoostingClassifier(loss='huber', n_estimators=375, max_depth=3, learning_rate=0.02, random_state=0, max_features=2, alpha=0.95), {'n_estimators': [100,150,200]}),\n",
    "    ('xgb', XGBClassifier(), {'n_estimators': [100,150,200]}),\n",
    "\n",
    "    (\"K-Nearest Neighbors Classification\", KNeighborsClassifier(), {'n_neighbors': [3,5]}),\n",
    "    #(\"Neural Network Classification\", MLPClassifier(max_iter=1000)),  # Specify appropriate hyperparameters\n",
    "\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "  \n",
    "    name = model[0]\n",
    "    print(model[0])\n",
    "    model = model[1]\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), (name, model)])\n",
    "    #model.fit(trainX_prepared, trainY)\n",
    "    grids =  GridSearchCV(pipe, param_grid={}, scoring='f1', cv=3)\n",
    "    grids.fit(X, y)\n",
    "    \n",
    "    #print('train result', result['test_score'])\n",
    "    grids.best_score_\n",
    "    # print('mean_score:', -np.mean(result['test_score']))\n",
    "    y_preds = grids.best_estimator_.predict(X_test)\n",
    "\n",
    "    print(f1_score(y_test, y_preds))\n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
